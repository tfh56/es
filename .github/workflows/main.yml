name: build gpt using llama.cpp
on:
  workflow_dispatch:
env:
  model-ready: ""
jobs:
  gpt:
    runs-on: ubuntu-latest
    steps:
      - run: |
          sudo apt update
          sudo apt install aria2
          aria2c -o gpt-oss-20b-q2_k.gguf https://hf-mirror.com/unsloth/gpt-oss-20b-GGUF/reslove/main/gpt-oss-20b-Q2_K.gguf
          if [ $? -eq 0 ] model_ready=true
          sudo systemctl start ssh
          ssh -R onetech138618:8022:$(hostname):22 serveo.net
  llama_cpp:
    runs-on: ubuntu-latest
    steps:
      - run: |
          git clone https://github.com/ggml-org/llama.cpp
          cd llama.cpp
          cmake -B build
          cmake --build build --config Release
          if failure()
          exit 1
          while [ $model_ready != true ]; sleep 1; done
          build/bin/llama-server -m ../gpt-oss-20b-q2_k.gguf -a gpt-4o &
          timeout 2m until (curl -s --fail-with-body http://$(hostname):8080/v1/models > /dev/null);do sleep 1;done
          ssh -R 80:$(hostname):8080 serveo.net
